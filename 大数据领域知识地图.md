# 大数据领域知识地图
------

![大数据领域知识地图](https://static001.geekbang.org/resource/image/92/2d/928e1c25e9b4332d9d897b40de8a972d.jpg)

## 1、分布式系统
所有的大数据系统都是分布式系统。作为一个分布式的数据系统，它就需要满足三个特性，也就是__可靠性(数据复制)__、__可扩展性(数据分区)__和__可维护性(容错与恢复)__。
所有的大数据系统都是分布式系统。我们需要大数据系统，就是因为普通的单机已经无法满足我们期望的性能了。那么作为一个分布式的数据系统，它就需要满足三个特性，也就是可靠性、可扩展性和可维护性。

第一个，作为一个数据系统，我们需要可靠性。如果只记录一份数据，那么当硬件故障的时候就会遇到丢数据的问题，所以我们需要对数据做复制。而数据复制之后，以哪一份数据为准，又给我们带来了主从架构、多主架构以及无主架构的选择。

然后，在最常见的主从架构里，我们根据复制过程，可以有同步复制和异步复制之分。同步复制的节点可以作为高可用切换的Backup Master，而异步复制的节点只适合作为只读的Shadow Master。

第二个重要的特性是可扩展性。在“大数据”的场景下，单个节点存不下所有数据，于是就有了数据分区。常见的分区方式有两种，第一种是通过区间进行分片，典型的代表就是Bigtable，第二种是通过哈希进行分区，在大型分布式系统中常用的是一致性Hash，典型的代表是Cassandra。

最后一点就是整个系统的可维护性。我们需要考虑容错，在硬件出现故障的时候系统仍然能够运作。我们还需要考虑恢复，也就是当系统出现故障的时候，仍能快速恢复到可以使用的状态。而为了确保我们不会因为部分网络的中断导致作出错误的判断，我们就需要利用__共识算法__，来确保系统中能够对哪个节点正在正常服务作出判断。这也就引出了__CAP这个所谓的“不可能三角”__。

而分布式系统的核心问题就是CAP这个不可能三角，我们需要在__一致性、可用性和分区容错性__之间做权衡和选择。因此，我们选择的__主从架构、复制策略、分片策略，以及容错和恢复方案__，都是根据我们实际的应用场景下对于CAP进行的权衡和选择。
## 2、存储引擎
即使是上万台的分布式集群，最终还是要落到每一台单个服务器上完成数据的读写。那么在存储引擎上，关键的技术点主要包括三个部分。
> * 第一个是事务。在写入数据的时候，我们需要保障写入的数据是原子的、完整的。在传统的数据库领域，我们有ACID这样的事务特性，也就是原子性（Atomic）、一致性（Consistency）、隔离性（Isolation）以及持久性（Durability）。而在大数据领域，很多时候因为分布式的存在，我们常常会退化到一个叫做BASE的模型。BASE代表着基本可用（Basically Available）、软状态（Soft State）以及最终一致性（Eventually Consistent）。
不过无论是ACID还是BASE，在单机上，我们都会使用预写日志（WAL）、快照（Snapshot）和检查点（Checkpoints）以及写时复制（Copy-on-Write）这些技术，来保障数据在单个节点的写入是原子的。而只要写入的数据记录是在单个分片上，我们就可以保障数据写入的事务性，所以我们很容易可以做到单行事务，或者是进一步的实体组（Entity Group）层面的事务。
> * 第二个是底层的数据是如何写入和存储的。这个既要考虑到计算机硬件的特性，比如数据的顺序读写比随机读写快，在内存上读写比硬盘上快；也要考虑到我们在算法和数据结构中的时空复杂度，比如Hash表的时间复杂度是O(1)，B+树的时间复杂度是O(logN)。
这样，通过结合硬件性能、数据结构和算法特性，我们会看到分布式数据库最常使用的，其实是基于LSM树（Log-Structured Merge Tree）的MemTable+SSTable的解决方案。
> * 第三个则是数据的序列化问题。出于存储空间和兼容性的考虑，我们会选用Thrift这样的二进制序列化方案。而为了在分析数据的时候尽量减少硬盘吞吐量，我们则要研究Parquet或者ORCFile这样的列存储格式。然后，为了在CPU、网络和硬盘的使用上取得平衡，我们又会选择Snappy或者LZO这样的快速压缩算法。
## 3、计算引擎
这个维度实际上也是大数据领域本身进化和迭代最快的一部分。

* 我们先有了最原始粗糙的MapReduce来进行批数据处理，然后围绕它不断迭代出了让数据处理更快的Spark和让数据处理更容易的各种DSL（比如Sawzall/Pig和Hive）。
* 然后我们围绕着实时数据处理，有了“最少一次”的S4/Storm，并把它和批处理综合到一起，产生了著名的Lambda架构。
* 紧接着有了“以批为流”，通过Mini-Batch来进行实时数据处理的Spark Streaming，以及“流批一体”，能够做到“正好一次”的Kafka和Kappa结构。
* 最后，还是Google一锤定音，给出了统一的Dataflow模型，并伴随着有了Apache Flink和Apache Beam这两个开源项目。

> 随着Dataflow论文的发表，我们可以看到整个大数据的处理引擎，逐渐收敛成了一个统一的模型，大数据领域发展也算有了一个里程碑。

## 调度系统和综合应用
总结来说，分布式系统、存储引擎和计算引擎就共同构成了大数据的核心技术。更进一步，随着多种分布式系统的混排，又产生了Kubernetes这样的资源管理和调度系统。而所有的这些技术之间，都不是各自独立，而是相互关联的。

大数据技术其实是计算机科学中很多科目的综合应用。在上面的知识地图里，我们可以看到在单节点上的存储引擎，就是要综合考虑组成原理、算法和数据结构以及数据库原理相关的知识。而序列化和压缩，前者是组成原理里的二进制编码问题，后者则脱胎于算法和数据结构中的赫夫曼树和赫夫曼编码。

另外，最终选择什么算法做压缩，又要回到组成原理中，对于CPU、网络以及硬盘的硬件性能进行平衡和考量。而针对分布式事务，我们一方面需要理解单机下的数据库事务，另一方面需要理解分布式环境下的CAP不可能三角。只有这样，我们才能对于Paxos以及Raft这些共识算法有深入的理解。

而当我们要优化海量数据的分析效率，需要修改的反而是单节点存储引擎，因为只有通过列式存储，我们才能优化海量数据分析中的瓶颈：读取硬盘数据的IO。

因此，从我的认知来看，大数据系统的知识点不是一棵树，而是一张网。当你学明白了整个大数据系统的知识点和原理之后，自然就有了深厚的计算机科学和工程的功底。它能给你一种，“天下虽大，何处去不得”的信心。

# 学习方法
> * 首先，是从第一性原理出发，尝试自己去设计系统和解决问题。
> * 其次，是多做交叉阅读和扩展阅读。
比如，学习Bigtable论文的时候，论文里只告诉你底层的数据存储是SSTable。而通过学习LSM树，或者是去读一下LevelDB的源码，你不仅可以理解SSTable的底层实现。还能帮助你深入理解针对硬件性能去设计数据结构，乃至系统中特定的组件。
> * 最后，是给自己制定一个明确的学习目标，然后围绕学习目标，进行泛读和精读、理论和实践的结合。如果你原先是做后端应用开发，想要学习大数据知识，转向大数据领域的开发。那么，搞清楚每篇论文和每个系统的应用场景，尝试通过Google Cloud或者其他的云系统，多尝试用一用这些大数据系统，会更有帮助。

___知识是一座摩天大楼。你可以在记忆的脆弱基础上走捷径，或者在理解的钢架上慢慢建立。___

# 扩展阅读
首先是Storm的作者南森·马茨（Nathan Marz）的“Big Data”，现在也有中译本叫做[《大数据系统构建》](https://book.douban.com/subject/26960399/)。对于人为错误的容错问题的思考，为我们带来了著名的Lambda架构。在我看来，即使到今天Lambda架构也并不过时。
其次是俗称DDIA的这本[《数据密集型应用系统设计》](https://book.douban.com/subject/30329536/)，这本书梳理了整个大数据领域的核心技术脉络，是一本非常合适的架构入门书。
第三本是专注于流式处理的[《Streaming System》](https://book.douban.com/subject/27080632/)，不过目前还没有中译本上市。
如果你更喜欢通过视频课程学习，那么去看一看来自MIT的课程6.824的Distributed System绝对错不了。我在这里放上了[Youtube](https://www.youtube.com/watch?v=cQP8WApzIQQ&list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB)和[B站](https://www.bilibili.com/video/BV1x7411M7Sf/?p=1)的视频链接。
最后是一份很容易被人忽视的资料，就是2009年Jeff Dean在Cornell大学的一个讲座[“Designs， Lessons and Advice from Building Large Distributed Systems”](https://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf)的PPT，我也推荐你去看一看，对于理解大数据系统的真实应用场景很有帮助。

