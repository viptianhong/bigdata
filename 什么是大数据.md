# 什么是大数据
------
>* __能够伸缩到一千台服务器以上的分布式数据处理集群的技术__
>* __这个上千个节点的集群，是采用廉价的PC架构搭建起来的__
>* __把数据中心当作是一台计算机__
以上这些变化，也给我们带来了大量新的技术挑战。而解决了这些挑战（可用性、数据一致性等）的种种技术方案，就是我们的“大数据”技术。

Bigtable不支持跨行事务也不支持SQL，5年后发表的Megastore，才开始着手解决这两个问题。

面对存储、计算和在线服务这三个需求，Google就在2003、2004以及2006年，分别抛出了三篇重磅论文。“大数据”的三驾马车：GFS、MapReduce和Bigtable。

无论是_GFS存储数据_，还是_MapReduce处理数据_，系统的吞吐量都没有问题了，因为所有的数据都是__顺序读写__。但是这两个，其实都没有办法解决好数据的高性能随机读写问题。面对这个问题，2006年发表的Bigtable就站上了历史舞台了。它是直接使用GFS作为底层存储，来做好集群的分片调度，以及利用MemTable+SSTable的底层存储格式，来解决大集群、机械硬盘下的高性能的__随机读写__问题。

![谷歌三驾马车](https://static001.geekbang.org/resource/image/e0/32/e069a97c337d583yyddb87fe51992232.jpg)

* 保障数据一致性的分布式锁，Google在发表Bigtable的同一年，就发表了实现了Paxos算法的Chubby锁服务的论文。
* 数据序列化以及分布式系统之间通信，Google在前面的论文里都没有提到这一点，Facebook在2007年发表Thrift的相关论文。
> 小知识：实际上，Bigtable的开源实现HBase，就用了Thrift作为和外部多语言进行通信的协议。Twitter也开源了elephant-bird，使得Hadoop上的MapReduce可以方便地使用Thrift来进行数据的序列化。

### MapReduce计算引擎的进化
* __阶段1-编程模型__：MapReduce的编程模型还是需要工程师去写程序的，所以它进化的方向就是通过一门DSL，进一步降低写MapReduce的门槛。虽然Google发表了Sawzall，Yahoo实现了Pig，但是在这个领域的第一阶段最终胜出的，是Facebook在2009年发表的Hive。Hive通过一门基本上和SQL差不多的HQL，大大降低了数据处理的门槛，从而成为了大数据数据仓库的事实标准。
* __阶段2-执行引擎__：Hive虽然披上了一个SQL的皮，但是它的底层仍然是一个个MapReduce的任务，所以延时很高，没法当成一个交互式系统来给数据分析师使用。于是Google又在2010年，发表了_Dremel_这个交互式查询引擎的论文，采用_数据列存储+并行数据库_的方式。这样一来，Dremel不仅有了一个SQL的皮，还进一步把MapReduce这个执行引擎给替换掉了。
* __最后是多轮迭代问题__：在MapReduce这个模型里，一个MapReduce就要读写一次硬盘，而且Map和Reduce之间的数据通信，也是先要落到硬盘上的。这样，无论是复杂一点的Hive SQL，还是需要进行上百轮迭代的机器学习算法，都会浪费非常多的硬盘读写。于是和Dremel论文发表的同一年，来自Berkeley的博士生马泰·扎哈里亚（Matei Zaharia），就发表了Spark的论文，通过把数据放在内存而不是硬盘里，大大提升了分布式数据计算性能。

所以到这里，你可以看到，围绕MapReduce，整个技术圈都在不断优化和迭代计算性能，Hive、Dremel和Spark分别从“更容易写程序”“查询响应更快”“更快的单轮和多轮迭代”的角度，完成了对MapReduce的彻底进化。

### Bigtable分布式存储（在线服务数据库）的进化
* __阶段1-事务问题和Schema问题__：Google先是在2011年发表了Megastore的论文，在Bigtable之上，实现了类SQL的接口，提供了Schema，以及简单的跨行事务。如果说Bigtable为了伸缩性，放弃了关系型数据库的种种特性。那么Megastore就是开始在Bigtable上逐步弥补关系型数据库的特性。
* __阶段2-异地多活和跨数据中心问题__：Google在2012年发表的Spanner，能够做到“全局一致性”。这样，就算是基本解决了这两个问题，第一次让我们有一个“全球数据库”。

![批计算和存储进化过程](https://static001.geekbang.org/resource/image/cc/d5/ccc40c7c9770f7e82594cb9d5dd399d5.jpg)

### 实时数据处理的抽象进化
从MapReduce到Dremel，我们查询数据的响应时间就大大缩短了。但是计算的数据仍然是固定的、预先确定的数据，这样系统往往有着大到数小时、小到几分钟的数据延时。为了解决好这个问题，流式数据处理就走上了舞台。
> * 首先是Yahoo在2010年发表了S4的论文，并在2011年开源了S4。
> * 而几乎是在同一时间，Twitter工程师南森·马茨（Nathan Marz）以一己之力开源了Storm，并且在很长一段时间成为了工业界的事实标准。和GFS一样，Storm还支持“至少一次”（At-Least-Once）的数据处理。另外，基于Storm和MapReduce，南森更是提出了__Lambda架构__，它可以称之为是第一个“__流批协同__”的大数据处理架构。
> * 接着在2011年，Kafka的论文也发表了。最早的Kafka其实只是一个“消息队列”，看起来它更像是Scribe这样进行数据传输组件的替代品。但是由于Kafka里发送的消息可以做到“正好一次”（Exactly-Once），所以大家就动起了在上面直接解决Storm解决不好的消息重复问题的念头。于是，Kafka逐步进化出了Kafka Streams这样的实时数据处理方案。而后在2014年，Kafka的作者Jay Krepson提出了__Kappa架构__，这个可以被称之为__第一代__“__流批一体__”的__大数据处理架构__。
> * 大数据的流式处理似乎没有Google什么事儿。的确，在流式数据处理领域，Google发表的FlumeJava和MillWheel的论文，并没有像前面的三驾马车或者Spanner的影响力那么大。但是在2015年，Google发表的__Dataflow的模型__，可以说是对于流式数据处理模型做出了最好的总结和抽象。一直到现在，Dataflow就成为了__真正的__“__流批一体__”的__大数据处理架构__。而后来开源的__Flink__和__Apache Beam__，则是完全按照Dataflow的模型实现的了。


![流计算进化过程](https://static001.geekbang.org/resource/image/0f/b9/0f55142af70b3f40fa5b9b8a3f24c9b9.jpg)

![论文脉络](https://static001.geekbang.org/resource/image/a8/1f/a898bc57b976a8a6e10b84507c4ce81f.jpg)

### 将所有服务器放在一起的资源调度
到了现在，随着“大数据领域”本身的高速发展，数据中心里面的服务器越来越多，我们对于数据___一致性__的要求也越来越高。那么，为了解决一致性问题，我们就有了基于Paxos协议的分布式锁。但是Paxos协议的性能很差，于是有了进一步的Multi-Paxos协议。而接下来的问题就是，Paxos协议并不容易理解，于是就有了Raft这个更容易理解的算法的出现。__Kubernetes依赖的etcd__就是用__Raft协议__实现的，我们在后面的资源调度篇里，会一起来看一下Raft协议到底是怎么实现的，以及现代分布式系统依赖的基础设施是什么样子的。然后，也正是因为数据中心里面的服务器越来越多，我们会发现原有的系统部署方式越来越浪费。原先我们一般是一个计算集群独占一系列服务器，而往往很多时候，我们的服务器资源都是闲置的。这在服务器数量很少的时候确实不太要紧，但是，当我们有数百乃至数千台服务器的时候，浪费的硬件和电力成本就成为不能承受之重了。于是，尽可能用满硬件资源成为了刚需。由此一来，我们对于整个分布式系统的视角，也从虚拟机转向了__容器__，这也是__Kubernetes(K8s)__这个系统的由来。在后面的资源调度篇中，我们就会一起来深入看看，Kubernetes这个更加抽象、全面的__资源管理和调度系统__。

![论文索引](https://static001.geekbang.org/resource/image/fd/e0/fdc694d707095e8d73dc18521676c0e0.jpg)

上面提到的这十几篇论文，其实只是2003到2015年这12年的大数据发展的冰山一角。还有许许多多值得一读的论文，比如针对Bigtable，你就可以还去读一下Cassandra和Dynamo，这样思路略有不同的分布式数据的论文；针对Borg和Kubernetes，你可以去看看Mesos这个调度系统的论文又是什么样的。网上更有“开源大数据架构的100篇论文”这样的文章，如果你想深耕大数据领域，也可以有选择地多读一些其中的论文。

[Big Data: ASurvey](http://www.cs.unibo.it/~danilo.montesi/CBD/Articoli/SurveyBigData.pdf)




